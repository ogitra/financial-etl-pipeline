# üß© Pipeline ETL de Demonstra√ß√µes Financeiras
üîó [- English (Short Version)](README.en.md)



## ‚ö° Resumo R√°pido do Projeto e Habilidades Demonstradas

**Este projeto demonstra, de ponta a ponta, um pipeline ETL aplicado a dados financeiros reais ‚Äî abrangendo desde consultas SQL at√© a modelagem dimensional e o c√°lculo de indicadores, culminando na carga em um Data Warehouse (SQLite) e em um Data Lake (Amazon S3).**


**O que o pipeline faz:**
- Extrai dados cont√°beis estruturados via SQL robusto (CTEs + Window Functions).
- Padroniza e transforma dados brutos em modelos dimensionais (dimens√µes + fato).
- Gera tabela anal√≠tica consolidada (wide table) para an√°lises.
- Calcula indicadores financeiros essenciais (liquidez, rentabilidade, endividamento, caixa) e evolu√ß√£o temporal (YoY).
- Carrega os datasets finais em:
  - **SQLite** (Data Warehouse local)
  - **Amazon S3** (Data Lake)


**Compet√™ncias demonstradas:**
- SQL avan√ßado (CTEs, janelas, agrega√ß√µes, filtros condicionais).
- Python + Pandas aplicado a dados financeiros.
- Modelagem dimensional (dim_company, dim_account, fact_balance).
- Transforma√ß√µes anal√≠ticas (wide_table, indicadores e evolu√ß√£o).
- Integra√ß√£o com AWS (S3) como destino de dados.
- Arquitetura modular (extract ‚Üí transform ‚Üí load).
- Organiza√ß√£o profissional de projeto (estrutura, versionamento, reprodutibilidade).
---

# üìä Projeto ‚Äî Pipeline ETL de Demonstra√ß√µes Financeiras

Projeto baseado em **dados reais** de demonstra√ß√µes financeiras das **Top 10 empresas do setor de com√©rcio brasileiro por receita em 2024**.

O pipeline padroniza dados cont√°beis, modela dimens√µes e fato e calcula **indicadores financeiros fundamentais**, como:

- Liquidez (corrente, geral e imediata)
- Rentabilidade (ROE, ROA e margens)
- Estrutura de capital e endividamento
- Gera√ß√£o e convers√£o de caixa
- Evolu√ß√£o financeira ano a ano (YoY)

---

## ‚ú®  Destaque: Query SQL
üîó [Query SQL](sql/top10_empresas_comercio_receita_2024.sql)

A extra√ß√£o dos dados foi feita a partir de uma **query SQL completa, constru√≠da para filtrar, agregar e preparar as demonstra√ß√µes financeiras antes mesmo do ETL em Python**.
A query utiliza:

- **CTEs** para modularizar etapas (filtro por setor, ranking de receita, base cont√°bil).
- **Window Functions** como `ROW_NUMBER()` e `SUM() OVER(PARTITION BY...)`.
- **Filtros por ano** para criar dataset entre 2022 e 2024.
- **Jun√ß√µes entre tabelas cont√°beis** para consolidar informa√ß√µes.
- **Padroniza√ß√£o de colunas** para facilitar o Transform.

### Trecho ilustrativo (exemplo reduzido):

```sql
WITH balancos_normalizados AS (
    SELECT
        b.IdPessoaJuridica,
        b.DataFechamento,
        bc.IdConta,

        /* Padroniza√ß√£o monet√°ria */
        CASE
            WHEN um.IdUnidadeMonetaria = -3 THEN bc.Valor * 1000000
            WHEN um.IdUnidadeMonetaria = -2 THEN bc.Valor * 1000
            ELSE bc.Valor
        END AS ValorPadronizado,

        /* Regra de prioridade entre balan√ßos */
        ROW_NUMBER() OVER (
            PARTITION BY
                b.IdPessoaJuridica,
                b.DataFechamento,
                bc.IdConta
            ORDER BY
                CASE
                    WHEN b.IdNaturezaBalanco = -2 THEN 1   -- Consolidado
                    WHEN b.IdNaturezaBalanco = -1 THEN 2   -- Individual
                    ELSE 3
                END
        ) AS rn

    FROM Balanco b
    JOIN BalancoConta bc
        ON bc.IdBalanco = b.IdBalanco
    JOIN UnidadeMonetaria um
        ON b.IdUnidadeMonetaria = um.IdUnidadeMonetaria

    WHERE
        b.IdPeriodoBalanco = -3                         -- balan√ßo anual
        AND b.DataFechamento BETWEEN '2022-12-31' AND '2024-12-31'
        AND bc.IdConta IN (
            55, 56, 57, 73, 103,
            178, 230, 286,
            332, 333, 335, 347, 375,
            421, 462, 473
        )
        AND bc.Valor IS NOT NULL
        AND bc.Valor <> 0
),

```
---
## ‚ú®  Destaque ‚Äî Python
üîó [Orquestrador Pipeline](src/pipeline.py)

O pipeline utiliza Python de forma modular e organizada, cobrindo pr√°ticas valorizadas no mercado de dados:

### ‚úì Arquitetura e organiza√ß√£o
- Estrutura em **m√≥dulos independentes** (`extract`, `transform`, `load`)
- Script **orquestrador** (`pipeline.py`)
- Separa√ß√£o clara de responsabilidades (clean code aplicado)

### ‚úì Processamento e manipula√ß√£o de dados
- Uso de **Pandas** para padroniza√ß√£o, limpeza e transforma√ß√£o
- Convers√£o de tipos, parsing de datas e valida√ß√£o de schema
- Cria√ß√£o de **tabelas fato** e **dimens√µes** a partir de dataframes
- Camada anal√≠tica dentro do Transform (transform/analytics)

### ‚úì Integra√ß√£o com destino de dados
- SQLite (Data Warehouse local) para persistir as tabelas finais
- Amazon S3 (Data Lake) para armazenar os datasets finais como objetos CSV
- Automa√ß√£o de todo o fluxo com um √∫nico comando (`python pipeline.py`)


---


## üß± Arquitetura do Pipeline

```text
Query SQL (CTEs + Window Functions + Filtros Anuais)
   |
   v
CSV Bruto (sample versionado em data/raw)
   |
   v
Extract (DataFrame)
   |
   v
Transform
   |-- Padroniza√ß√£o (standardize)
   |-- Modelagem (dim_company, dim_account, fact_balance)
   |-- Wide Table (wide_table)
   |-- Analytics (KPIs e evolu√ß√£o)
   |
   v
Load (SQLite Data Warehouse)
   |-- SQLite (DW local)
   |-- S3 (Data Lake)

```

---

## üì¶ Outputs gerados
 O pipeline gera os seguintes datasets finais:

- dim_company
- dim_account
- fact_balance
- wide_table
- financial_indicators
- financial_evolution

## üóÑÔ∏è SQLite (Data Warehouse local)

O arquivo √© gerado automaticamente em:
data/warehouse/balance_dw.db (n√£o versionado no Git)

## ‚òÅÔ∏è Amazon S3 (Data Lake)

Os datasets s√£o enviados como CSV para o bucket configurado no loader do S3, organizados por camadas:
processed/dimensions/dim_company/dim_company.csv
processed/dimensions/dim_account/dim_account.csv
processed/facts/fact_balance/fact_balance.csv
processed/wide/wide_table.csv
analytics/financial_indicators.csv
analytics/financial_evolution.csv

---

## üìÅ Estrutura do Projeto

```text
data/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ top10_empresas_comercio_receita_2024.csv   # sample (input)
‚îú‚îÄ‚îÄ warehouse/
‚îÇ   ‚îî‚îÄ‚îÄ balance_dw.db                             # gerado em runtime (n√£o versionado)

sql/
‚îî‚îÄ‚îÄ top10_empresas_comercio_receita_2024.sql

src/
‚îú‚îÄ‚îÄ extract/
‚îú‚îÄ‚îÄ transform/
‚îÇ   ‚îî‚îÄ‚îÄ analytics/
‚îú‚îÄ‚îÄ load/
‚îÇ   ‚îú‚îÄ‚îÄ sqlite_loader.py
‚îÇ   ‚îî‚îÄ‚îÄ s3_loader.py
‚îú‚îÄ‚îÄ utils/
‚îî‚îÄ‚îÄ pipeline.py
```

---

## ‚ñ∂ Etapas do Pipeline

### 1. Extract
- CSV gerado previamente via query SQL avan√ßada
- Leitura do CSV em DataFrame

### 2. Transform
- Padroniza√ß√£o de colunas e tipos
- Cria√ß√£o de dimens√µes (empresa, conta, data)
- Constru√ß√£o da tabela fato
- Gera√ß√£o de tabela wide
- C√°lculo de indicadores financeiros e evolu√ß√£o temporal

### 3. Load
- Carga em SQLite
- Upload dos datasets finais para o S3 da AWS

---




## ‚ñ∂ Como Reproduzir o Projeto

Este projeto pode ser reproduzido localmente utilizando o arquivo CSV disponibilizado na pasta `data/raw`.

### Pr√©-requisitos
- Python 3.10 ou superior
- Git
- (Opcional para S3) AWS CLI configurado e bucket S3 criado
### Passo a passo

#### 1. Clone o reposit√≥rio
```bash
git clone https://github.com/ogitra/financial-etl-pipeline.git
cd financial-etl-pipeline
```

#### 2. Crie e ative um ambiente virtual
```bash
python -m venv venv
# Linux / Mac
source venv/bin/activate
# Windows
venv\Scripts\activate
```

#### 3. Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

#### 4. (Opcional) Configurar AWS para upload no S3
- Crie um bucket S3
- Crie um usu√°rio IAM com permiss√£o m√≠nima (s3:PutObject e s3:ListBucket)
- Configure as credenciais localmente:

*Se voc√™ quiser executar apenas o SQLite (sem AWS), basta comentar a chamada do loader S3 no pipeline.py.*



#### 5. Execute o pipeline ETL

O pipeline deve ser executado **a partir do diret√≥rio `src`**:

```bash
cd src
python pipeline.py
```


### Resultados esperados

Isso executar√° todas as etapas do pipeline:

- Extract
- Transform
- Load (SQLite + S3)



---

## ‚Ñπ Observa√ß√µes

- Cada etapa do pipeline possui responsabilidade clara e m√≥dulos separados.
- O projeto assume a estrutura de dados fornecida no arquivo CSV localizado em `data/raw`.
- O banco SQLite √© gerado automaticamente durante a execu√ß√£o e n√£o √© versionado no reposit√≥rio.

---

## ‚úî Status do Projeto
Projeto finalizado e desenvolvido para fins de estudo e portf√≥lio t√©cnico.
