# üß© Pipeline ETL de Demonstra√ß√µes Financeiras
üîó [- English (Short Version)](README.en.md)



## ‚ö° Resumo R√°pido do Projeto e Habilidades Demonstradas

**Este projeto demonstra, de ponta a ponta, um pipeline ETL aplicado a dados financeiros reais ‚Äî abrangendo desde consultas SQL at√© a modelagem dimensional e o c√°lculo de indicadores, culminando na carga em um Data Warehouse.**

**O que o pipeline faz:**
- Extrai dados cont√°beis estruturados via SQL robusto (CTEs + Window Functions).
- Padroniza e transforma dados brutos em modelos dimensionais (dimens√µes + fato).
- Calcula indicadores financeiros essenciais (liquidez, rentabilidade, endividamento, caixa).
- Gera tabela anal√≠tica consolidada para visualiza√ß√£o e an√°lises.
- Carrega tudo em um Data Warehouse SQLite.

**Compet√™ncias demonstradas:**
- SQL avan√ßado (CTEs, janelas, agrega√ß√µes, filtros condicionais).
- Python + Pandas aplicado a dados financeiros.
- Modelagem dimensional (dim_empresa, dim_conta, fato_financeiro).
- Arquitetura ETL modular (extract ‚Üí transform ‚Üí analytics ‚Üí load).
- L√≥gica financeira aplicada (ROE, ROA, margens, liquidez, endividamento).
- Organiza√ß√£o profissional de projeto (estrutura, versionamento, reprodutibilidade).

---

# üìä Projeto ‚Äî Pipeline ETL de Demonstra√ß√µes Financeiras

Projeto baseado em **dados reais** de demonstra√ß√µes financeiras das **Top 10 empresas do setor de com√©rcio brasileiro por receita em 2024**.

O pipeline padroniza dados cont√°beis, modela dimens√µes e fato e calcula **indicadores financeiros fundamentais**, como:

- Liquidez (corrente, geral e imediata)
- Rentabilidade (ROE, ROA e margens)
- Estrutura de capital e endividamento
- Gera√ß√£o e convers√£o de caixa
- Evolu√ß√£o financeira ano a ano (YoY)

---

## ‚ú®  Destaque: Query SQL
üîó [Query SQL](sql/top10_empresas_comercio_receita_2024.sql)

A extra√ß√£o dos dados foi feita a partir de uma **query SQL completa, constru√≠da para filtrar, agregar e preparar as demonstra√ß√µes financeiras antes mesmo do ETL em Python**.
A query utiliza:

- **CTEs** para modularizar etapas (filtro por setor, ranking de receita, base cont√°bil).
- **Window Functions** como `ROW_NUMBER()` e `SUM() OVER(PARTITION BY...)`.
- **Filtros por ano** para criar dataset entre 2022 e 2024.
- **Jun√ß√µes entre tabelas cont√°beis** para consolidar informa√ß√µes.
- **Padroniza√ß√£o de colunas** para facilitar o Transform.

### Trecho ilustrativo (exemplo reduzido):

```sql
WITH balancos_normalizados AS (
    SELECT
        b.IdPessoaJuridica,
        b.DataFechamento,
        bc.IdConta,

        /* Padroniza√ß√£o monet√°ria */
        CASE
            WHEN um.IdUnidadeMonetaria = -3 THEN bc.Valor * 1000000
            WHEN um.IdUnidadeMonetaria = -2 THEN bc.Valor * 1000
            ELSE bc.Valor
        END AS ValorPadronizado,

        /* Regra de prioridade entre balan√ßos */
        ROW_NUMBER() OVER (
            PARTITION BY
                b.IdPessoaJuridica,
                b.DataFechamento,
                bc.IdConta
            ORDER BY
                CASE
                    WHEN b.IdNaturezaBalanco = -2 THEN 1   -- Consolidado
                    WHEN b.IdNaturezaBalanco = -1 THEN 2   -- Individual
                    ELSE 3
                END
        ) AS rn

    FROM Balanco b
    JOIN BalancoConta bc
        ON bc.IdBalanco = b.IdBalanco
    JOIN UnidadeMonetaria um
        ON b.IdUnidadeMonetaria = um.IdUnidadeMonetaria

    WHERE
        b.IdPeriodoBalanco = -3                         -- balan√ßo anual
        AND b.DataFechamento BETWEEN '2022-12-31' AND '2024-12-31'
        AND bc.IdConta IN (
            55, 56, 57, 73, 103,
            178, 230, 286,
            332, 333, 335, 347, 375,
            421, 462, 473
        )
        AND bc.Valor IS NOT NULL
        AND bc.Valor <> 0
),

```
---
## ‚ú®  Destaque ‚Äî Python
üîó [Orquestrador Pipeline](src/pipeline.py)

O pipeline utiliza Python de forma modular e organizada, cobrindo pr√°ticas valorizadas no mercado de dados:

### ‚úì Arquitetura e organiza√ß√£o
- Estrutura em **m√≥dulos independentes** (`extract`, `transform`, `analytics`, `load`)
- Script **orquestrador** (`pipeline.py`)
- Separa√ß√£o clara de responsabilidades (clean code aplicado)

### ‚úì Processamento e manipula√ß√£o de dados
- Uso de **Pandas** para padroniza√ß√£o, limpeza e transforma√ß√£o
- Convers√£o de tipos, parsing de datas e valida√ß√£o de schema
- Cria√ß√£o de **tabelas fato** e **dimens√µes** a partir de dataframes

### ‚úì Boas pr√°ticas de Engenharia de Dados
- Diret√≥rios por est√°gio (`raw`, `extract`, `standardized`, ‚Ä¶)
- Estrutura de *staging ‚Üí curated ‚Üí analytics*
- Scripts reutiliz√°veis dentro de `utils/`
- L√≥gica de transforma√ß√£o separada do c√≥digo de carga

### ‚úì Integra√ß√£o com banco de dados
- Escrita das tabelas finais em **SQLite** (Data Warehouse local)
- Cria√ß√£o de tabelas e inser√ß√£o de dados via Pandas + SQL engine
- Automa√ß√£o de todo o fluxo com um √∫nico comando (`python pipeline.py`)

Esses pontos refletem pr√°ticas consideradas padr√£o em pipelines ETL de Data Engineering, mesmo em ambientes maiores (AWS, GCP, Databricks), s√≥ adaptadas para um projeto local.




---

## üß± Arquitetura do Pipeline

```text
Query SQL (CTEs + Window Functions + Filtros Anuais)
   |
   v
CSV Bruto
   |
   v
Extract
   |
   v
Transform
   |-- Dados Padronizados
   |-- Dimens√µes e Fato
   |
   v
Analytics
   |-- Tabela Wide
   |-- Indicadores Financeiros
   |-- Evolu√ß√£o Temporal
   |
   v
Load (SQLite Data Warehouse)

```

---

## üìÅ Estrutura do Projeto

```text
data/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ top10_empresas_comercio_receita_2024.csv
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ extract/
‚îÇ   ‚îú‚îÄ‚îÄ standardized/
‚îÇ   ‚îú‚îÄ‚îÄ dimensions_fact/
‚îÇ   ‚îî‚îÄ‚îÄ analytics/
‚îú‚îÄ‚îÄ warehouse/
‚îÇ   ‚îî‚îÄ‚îÄ balance_dw.db

sql/
‚îî‚îÄ‚îÄ top10_empresas_comercio_receita_2024.sql

src/
‚îú‚îÄ‚îÄ extract/
‚îú‚îÄ‚îÄ transform/
‚îú‚îÄ‚îÄ analytics/
‚îú‚îÄ‚îÄ load/
‚îú‚îÄ‚îÄ utils/
‚îî‚îÄ‚îÄ pipeline.py
```

---

## ‚ñ∂ Etapas do Pipeline

### 1. Extract
- CSV gerado previamente via query SQL avan√ßada
- Leitura do CSV em DataFrame

### 2. Transform
- Padroniza√ß√£o de colunas e tipos
- Cria√ß√£o de dimens√µes (empresa, conta, data)
- Constru√ß√£o da tabela fato

### 3. Analytics
- Gera√ß√£o de tabela wide
- C√°lculo de KPIs financeiros
- An√°lise temporal (YoY)

### 4. Load
- Carga em SQLite
- Tabelas organizadas por tema (dimens√µes, fato, analytics)

---




## ‚ñ∂ Como Reproduzir o Projeto

Este projeto pode ser reproduzido localmente utilizando o arquivo CSV disponibilizado na pasta `data/raw`.

### Pr√©-requisitos
- Python 3.10 ou superior
- Git

### Passo a passo

#### 1. Clone o reposit√≥rio
```bash
git clone https://github.com/ogitra/financial-etl-pipeline.git
cd financial-etl-pipeline
```

#### 2. Crie e ative um ambiente virtual
```bash
python -m venv venv
# Linux / Mac
source venv/bin/activate
# Windows
venv\Scripts\activate
```

#### 3. Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

#### 4. Execute o pipeline ETL

O pipeline deve ser executado **a partir do diret√≥rio `src`**:

```bash
cd src
python pipeline.py
```


### Resultados esperados

Isso executar√° todas as etapas do pipeline:

- Extract
- Transform
- Analytics
- Load (SQLite)

Gerando as tabelas separadas por pastas.

---

## ‚Ñπ Observa√ß√µes

- Cada etapa do pipeline possui responsabilidade clara e m√≥dulos separados.
- O projeto assume a estrutura de dados fornecida no arquivo CSV localizado em `data/raw`.
- O banco SQLite √© gerado automaticamente durante a execu√ß√£o e n√£o √© versionado no reposit√≥rio.

---

## ‚úî Status do Projeto
Projeto finalizado e desenvolvido para fins de estudo e portf√≥lio t√©cnico.
